
# GFS（Google 文件系统）

GFS 是一个可扩展的[分布式文件系统](https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，并提供容错功能。它可以给大量的用户提供总体性能较高的服务。

GFS, google File System，[Google](https://baike.baidu.com/item/Google)公司为了存储海量搜索数据而设计的专用[文件系统](https://baike.baidu.com/item/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)。

## 设计概览

**⑴设计想定**

GFS与过去的[分布式文件系统](https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)有很多相同的目标，但GFS的设计受到了当前及预期的应用方面的工作量及技术环境的驱动，这反映了它与早期的文件系统明显不同的设想。这就需要对传统的选择进行重新检验并进行完全不同的设计观点的探索。

GFS与以往的文件系统的不同的观点如下：

⒈ 部件错误不再被当作异常，而是将其作为常见的情况加以处理。因为文件系统由成百上千个用于存储的机器构成，而这些机器是由廉价的普通部件组成并被大量的客户机访问。部件的数量和质量使得一些机器随时都有可能无法工作并且有一部分还可能无法恢复。所以实时地监控、错误检测、容错、自动恢复对系统来说必不可 少。

⒉按照传统的标准，文件都非常大。长度达几个GB的文件是很平常的。每个文件通常包含很多应用对象。当经常要处理快速增长的、包含数以 万计的对象、长度达TB的数据集时，我们很难管理成千上万的KB规模的文件块，即使底层文件系统提供支持。因此，设计中操作的参数、块的大小必须要重新考 虑。对大型的文件的管理一定要能做到高效，对小型的文件也必须支持，但不必优化。

⒊大部分文件的更新是通过添加新数据完成的，而不是改变已 存在的数据。在一个文件中随机的操作在实践中几乎不存在。一旦写完，文件就只可读，很多数据都有这些特性。一些数据可能组成一个大仓库以供数据分析程序扫 描。有些是运行中的程序连续产生的数据流。有些是档案性质的数据，有些是在某个机器上产生、在另外一个机器上处理的中间数据。由于这些对大型文件的访问方 式，添加操作成为性能优化和原子性保证的焦点。而在客户机中缓存[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)则失去了吸引力。

⒋工作量主要由两种读操作构成：对大量数据的流方式 的读操作和对少量数据的随机方式的读操作。在前一种读操作中，可能要读几百KB，通常达 1MB和更多。来自同一个客户的连续操作通常会读文件的一个连续的区域。随机的读操作通常在一个随机的偏移处读几个KB。性能敏感的[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)通常将对少量 数据的读操作进行分类并进行[批处理](https://baike.baidu.com/item/%E6%89%B9%E5%A4%84%E7%90%86)以使得读操作稳定地向前推进，而不要让它来反反复复地读。

⒌工作量还包含许多对大量数据进行的、连续的、向文件添加数据的写操作。所写的数据的规模和读相似。一旦写完，文件很少改动。在随机位置对少量数据的写操作也支持，但不必非常高效。

⒍系统必须高效地实现定义完好的大量客户同时向同一个文件的添加操作的语义。

7.高可持续带宽比低延迟更重要

**⑵系统接口**

GFS提供了一个相似地文件系统界面，虽然它没有像POSⅨ那样实现标准的API。文件在目录中按层次组织起来并由[路径](https://baike.baidu.com/item/%E8%B7%AF%E5%BE%84)名标识。

**⑶**[体系结构](https://baike.baidu.com/item/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84)

一 个GFS[集群](https://baike.baidu.com/item/%E9%9B%86%E7%BE%A4)由一个master和大量的chunkserver构成，并被许多客户（Client）访问。如图1所示。

[![img](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D250/sign=82a71fc2a044ad342abf8082e0a30c08/9825bc315c6034a84d5b05aeca13495409237667.jpg)](https://baike.baidu.com/pic/GFS/1813072/0/9825bc315c6034a84d5b05aeca13495409237667?fr=lemma&ct=single)

Master和 chunkserver通常是运行[用户层](https://baike.baidu.com/item/%E7%94%A8%E6%88%B7%E5%B1%82)服务进程的Linux机器。只要资源和可靠性允许，chunkserver和client可以运行在同一个机器 上。文件被分成固定大小的块。每个块由一个不变的、全局唯一的64位的chunk－handle标识，chunk－handle是在块创建时 由 master分配的。ChunkServer将块当作Linux文件存储在[本地磁盘](https://baike.baidu.com/item/%E6%9C%AC%E5%9C%B0%E7%A3%81%E7%9B%98)并可以读和写由chunk－handle和位区间指定的数据。出于可靠 性考虑，每一个块被复制到多个chunkserver上。默认情况下，保存3个副本，但这可以由用户指定。

Master维护文件系统所以的元 数据（metadata），包括名字空间、访问控制信息、从文件到块的映射以及块的当前位置。它也控制系统范围的活动，如块租约（lease）管理，孤儿 块的垃圾收集，chunkserver间的块迁移。Master定期通过HeartBeat消息与每一个 chunkserver通信，给chunkserver传递指令并收集它的状态。

与每个应用相联的GFS客户代码实现了文件系统的API并与master和chunkserver通信以代表[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)读和写数据。客户与master的交换只限于对元数据（metadata）的操作，所有数据方面的通信都直接和chunkserver联系。

客 户和chunkserver都不缓存文件数据。因为用户缓存的益处微乎其微，这是由于数据太多或[工作集](https://baike.baidu.com/item/%E5%B7%A5%E4%BD%9C%E9%9B%86)太大而无法缓存。不缓存数据简化了客户程序和整个系统，因为不必考虑缓存的一致性问题。但用户缓存元数据（metadata）。Chunkserver也不必缓存文件，因为块是作为本地文件存储的。

## GFS架构

GFS的新颖之处并不在于它采用了多么令人惊讶的新技术，而在于它采用廉价的商用计算机集群构建分布式文件系统，在降低成本的同时经受了实际应用的考验。

如上图所示，一个GFS包括一个主服务器（master）和多个块服务器（chunk server），这样一个GFS能够同时为多个客户端应用程序（Application）提供文件服务。文件被划分为固定的块，由主服务器安排存放到块服务器的本地硬盘上。主服务器会记录存放位置等数据，并负责维护和管理文件系统，包括块的租用、垃圾块的回收以及块在不同块服务器之间的迁移。此外，主服务器还周期性地与每个块服务器通过消息交互，以监视运行状态或下达命令。应用程序通过与主服务器和块服务器的交互来实现对应用数据的读写，应用与主服务器之间的交互仅限于元数据，也就是一些控制数据，其他的数据操作都是直接与块服务器交互的。这种控制与业务相分离的架构，在互联网产品方案上较为广泛，也较为成功。

**⑷单master**

只 有一个master也极大的简化了设计并使得master可以根据全局情况作出精密的块放置和复制决定。但是我们必须要将master对读和写的参与减至 最少，这样它才不会成为系统的瓶颈。Client从来不会从master读和写文件数据。Client只是询问master它应该和哪个 chunkserver联系。Client在一段限定的时间内将这些信息缓存，在后续的操作中Client直接和chunkserver交互。

[![img](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D220/sign=4b8a908ebaa1cd1101b675228913c8b0/a50f4bfbfbedab64b019397ef736afc379311e20.jpg)](https://baike.baidu.com/pic/GFS/1813072/0/a50f4bfbfbedab64b019397ef736afc379311e20?fr=lemma&ct=single)

⒈client使用固定的块大小将[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)指定的文件名和字节偏移转换成文件的一个块索引（chunk index）。

⒉给master发送一个包含文件名和块索引的请求。

⒊master回应对应的chunk handle和副本的位置（多个副本）。

⒋client以文件名和块索引为键缓存这些信息。（handle和副本的位置）。

⒌Client 向其中一个副本发送一个请求，很可能是最近的一个副本。请求指定了chunk handle（chunkserver以chunk handle标识chunk）和块内的一个字节区间。

⒍除非缓存的信息不再有效（cache for a limited time）或文件被重新打开，否则以后对同一个块的读操作不再需要client和master间的交互。

通常Client可以在一个请求中询问多个chunk的地址，而master也可以很快回应这些请求。

**⑸块规模**

块规模是设计中的一个关键参数。我们选择的是64MB，这比一般的文件系统的块规模要大的多。每个块的副本作为一个普通的Linux文件存储，在需要的时候可以扩展。

块规模较大的好处有：

⒈减少client和master之间的交互。因为读写同一个块只是要在开始时向master请求块位置信息。对于读写大型文件这种减少尤为重要。即使对于访问少量数据的随机读操作也可以很方便的为一个规模达几个TB的[工作集](https://baike.baidu.com/item/%E5%B7%A5%E4%BD%9C%E9%9B%86)缓缓存块位置信息。

⒉Client在一个给定的块上很可能执行多个操作，和一个chunkserver保持较长时间的TCP连接可以减少网络负载。

⒊这减少了master上保存的元数据（metadata）的规模，从而使得可以将metadata放在内存中。这又会带来一些别的好处。

不利的一面：

一个小文件可能只包含一个块，如果很多Client访问该文件的话，存储这些块的chunkserver将成为访问的热点。但在实际应用中，[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)通常顺序地读包含多个块的文件，所以这不是一个主要问题。

**⑹元数据（metadata）**

master 存储了三种类型的metadata：文件的名字空间和块的名字空间，从文件到块的映射，块的副本的位置。所有的metadata都放在内存中。前两种类型 的metadata通过向操作日志登记修改而保持不变，操作日志存储在master的[本地磁盘](https://baike.baidu.com/item/%E6%9C%AC%E5%9C%B0%E7%A3%81%E7%9B%98)并在几个远程机器上留有副本。使用[日志](https://baike.baidu.com/item/%E6%97%A5%E5%BF%97)使得我们可以很简单 地、可靠地更新master的状态，即使在master崩溃的情况下也不会有不一致的问题。相反，master在每次启动以及当有 chunkserver加入的时候询问每个chunkserver的所拥有的块的情况。

A、内存[数据结构](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84)：

因为metadata存储在内存中，所以master的操作很快。进一步，master可以轻易而且高效地定期在[后台](https://baike.baidu.com/item/%E5%90%8E%E5%8F%B0)扫描它的整个状态。这种定期地扫描被用于实现块垃圾收集、chunkserver出现故障时的副本复制、为平衡负载和磁盘空间而进行的块迁移。

这 种方法的一个潜在的问题就是块的数量也即整个系统的容量是否受限与master的内存。实际上，这并不是一个严重的问题。Master为每个 64MB的块维护的metadata不足64个字节。除了最后一块，文件所有的块都是满的。类似的，每个文件的名字[空间数据](https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE)也不足64个字节，因为文件名 是以一种事先确定的压缩方式存储的.如果要支持更大的文件系统，那么增加一些内存的方法对于我们将元数据（metadata）保存在内存中所获得的简单 性、可靠性、高性能和灵活性来说，这只是一个很小的代价。

B、块位置：

master并不为chunkserver所拥有的块的副本的保存一个不变的记录。它在启动时通过简单的查询来获得这些信息。Master可以保持这些信息的更新，因为它控制所有块的放置并通过HeartBeat消息来监控chunkserver的状态。

这样做的好处：因为chunkserver可能加入或离开集群、改变路径名、崩溃、重启等，一个集群中有成百个server，这些事件经常发生，这种方法就排除了master与chunkserver之间的同步问题。

另一个原因是：只有chunkserver才能确定它自己到底有哪些块，由于错误，chunkserver中的一些块可能会很自然的消失，这样在master中就没有必要为此保存一个不变的记录。

C、操作日志：

操作日志包含了对[metadata](https://baike.baidu.com/item/metadata)所作的修改的历史记录。它作为逻辑时间线定义了并发操作的执行顺序。文件、块以及它们的版本号都由它们被创建时的逻辑时间而唯一地、永久地被标识。

操作日志是如此的重要，我们必须要将它可靠地保存起来，并且只有在metadata的改变固定下来之后才将变化呈现给用户。所以我们将操作日志复制到数个远程的机器上，并且只有在将相应的日志记录写到本地和远程的磁盘上之后才回答用户的请求。

Master可以用操作日志来恢复它的文件系统的状态。为了将启动时间减至最小，日志就必须要比较小。每当[日志](https://baike.baidu.com/item/%E6%97%A5%E5%BF%97)的长度增长到超过一定的规模后，master就要检查它的状态，它可以从[本地磁盘](https://baike.baidu.com/item/%E6%9C%AC%E5%9C%B0%E7%A3%81%E7%9B%98)装入最近的检查点来恢复状态。

创 建一个检查点比较费时，master的内部状态是以一种在创建一个检查点时并不耽误即将到来的修改操作的方式来组织的。Master切换到一个新的日志文件并在一个单独的线程中创建检查点。这个新的检查点记录了切换前所有的修改。在一个有数十万文件的[集群](https://baike.baidu.com/item/%E9%9B%86%E7%BE%A4)中用一分钟左右就能完成。创建完后，将它写入本地和 远程的磁盘。

⑺[数据完整性](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7)

名字空间的修改必须是原子性的，它们只能有master处理：名字空间锁保证了操作的原子性和正确性，而master的操作日志在全局范围内定义了这些操作的顺序。

[![img](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D250/sign=81be544036d3d539c53d08c60a86e927/d4628535e5dde711d8bb3748a7efce1b9d166126.jpg)](https://baike.baidu.com/pic/GFS/1813072/0/d4628535e5dde711d8bb3748a7efce1b9d166126?fr=lemma&ct=single)

Write操作在[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)指定的偏移处写入数据，而record append操作使得数据（记录）即使在有并发修改操作的情况下也至少原子性的被加到GFS指定的偏移处，[偏移地址](https://baike.baidu.com/item/%E5%81%8F%E7%A7%BB%E5%9C%B0%E5%9D%80)被返回给用户。

在一系列成功的修改操作后，最后的修改操作保证文件区域是已定义的。GFS通过对所有的副本执行同样顺序的修改操作并且使用块版本号检测过时的副本（由于chunkserver退出而导致丢失修改）来做到这一点。

因为用户缓存了会位置信息，所以在更新缓存之前有可能从一个过时的副本中读取数据。但这有缓存的截止时间和文件的重新打开而受到限制。

在修改操作成功后，部件[故障](https://baike.baidu.com/item/%E6%95%85%E9%9A%9C)仍可以是数据受到破坏。GFS通过master和chunkserver间定期的handshake，借助校验和来检测对数据的破坏。一旦检测到，就从一个有效的副本尽快重新存储。只有在GFS检测前，所有的副本都失效，这个块才会丢失。

## 系统交互

**⑴租约（lease）和修改顺序**

**⑵**[数据流](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%B5%81)

我们的目标是充分利用每个机器的[网络带宽](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E5%B8%A6%E5%AE%BD)，避免[网络瓶颈](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%93%B6%E9%A2%88)和延迟

为了有效的利用网络，我们将数据流和[控制流](https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E6%B5%81)分离。数据是以流水线的方式在选定的chunkerserver链上线性的传递的。每个机器的整个对外带宽都被用作传递数据。为避免瓶颈，每个机器在收到数据后，将它收到数据尽快传递给离它最近的机器。

**⑶原子性的record Append**

GFS 提供了一个原子性的添加操作：record append。在传统的写操作中，client指定被写数据的偏移位置，向同一个区间的并发的写操作是不连续的：区间有可能包含来自多个client的数 据碎片。在record append中， client只是指定数据。GFS在其选定的偏移出将数据至少原子性的加入文件一次，并将偏移返回给client。

在分布式的应用中，不同机 器上的许多client可能会同时向一个文件执行添加操作，添加操作被频繁使用。如果用传统的write操作，可能需要额外的、复杂的、开销较大的同步， 例如通过分布式锁管理。在我们的工作量中，这些文件通常以多个生产者单个消费者队列的方式或包含从多个不同 client的综合结果。

Record append和前面讲的write操作的[控制流](https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E6%B5%81)差不多，只是在primary上多了一些逻辑判断。首先，client将数据发送到文件最后一块的所有副本 上。然后向primary发送请求。Primary检查添加操作是否会导致该块超过最大的规模（64M）。如果这样，它将该块扩充到最大规模，并告诉其它 副本做同样的事，同时通知client该操作需要在下一个块上重新尝试。如果记录满足最大规模的要求，primary就会将数据添加到它的副本上，并告诉 其它的副本在在同样的偏移处写数据，最后primary向client报告写操作成功。如果在任何一个副本上record append操作失败，client将重新尝试该操作。这时候，同一个块的副本可能包含不同的数据，因为有的可能复制了全部的数据，有的可能只复制了部 分。GFS不能保证所有的副本每个字节都是一样的。它只保证每个数据作为一个原子单元被写过至少一次。这个是这样得出的：操作要是成功，数据必须在所有的 副本上的同样的偏移处被写过。进一步，从这以后，所有的副本至少和记录一样长，所以后续的记录将被指定到更高的偏移处或者一个不同的块上，即使另一个副本 成了primary。根据一致性保证，成功的record append操作的区间是已定义的。而受到干扰的区间是不一致的。

**⑷**[快照](https://baike.baidu.com/item/%E5%BF%AB%E7%85%A7)（snapshot）

快照操作几乎在瞬间构造一个文件和目录树的副本，同时将正在进行的其他修改操作对它的影响减至最小。

我 们使用copy-on-write技术来实现snapshot。当master受到一个snapshot请求时，它首先将要snapshot的文件上块上 的lease收回。这使得任何一个向这些块写数据的操作都必须和master交互以找到拥有lease的副本。这就给master一个创建这个块的副本的机 会。

副本被撤销或终止后，master在[磁盘](https://baike.baidu.com/item/%E7%A3%81%E7%9B%98)上登记执行的操作，然后复制源文件或目录树的metadata以对它的内存状态实施登记的操作。这个新创建的snapshot文件和源文件（其metadata）指向相同的块（chunk）。

Snapshot 之后，客户第一次向chunk c写的时候，它发一个请求给master以找到拥有lease的副本。Master注意到chunk c的引用记数比1大，它延迟对用户的响应，选择一个chunk handle C’，然后要求每一有chunk c的副本的chunkserver创建一个块C’。每个chunkserver在本地创建chunk C’避免了网络开销。从这以后和对别的块的操作没有什么区别。

## MASTER操作

MASTER执行所有名字空间的操作，除此之外，他还在系统范围管理[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的复制：决定数据块的放置方案，产生新数据块并将其备份，和其他系统范围的操作协同来确保数据备份的完整性，在所有的数据块服务器之间平衡负载并收回没有使用的[存储空间](https://baike.baidu.com/item/%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4)。

**3.1 名字空间管理和加锁**

与传统文件系统不同的是，GFS没有与每个目录相关的能列出其所有文件的[数据结构](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84)，它也不支持别名（unix中的[硬连接](https://baike.baidu.com/item/%E7%A1%AC%E8%BF%9E%E6%8E%A5)或符号连接），不管是对文件或是目录。GFS的名字空间逻辑上是从文件元数据到路径名映射的一个查用表。

MASTER 在执行某个操作前都要获得一系列锁，例如，它要对/d1/d2…/dn/leaf执行操作，则它必须获得/d1，/d1/d2，…， /d1/d2/…/dn的读锁，/d1/d2…/dn/leaf的读锁或写锁（其中leaf可以使文件也可以是目录）。MASTER操作的[并行性](https://baike.baidu.com/item/%E5%B9%B6%E8%A1%8C%E6%80%A7)和数据的 一致性就是通过这些锁来实现的。

**3.2 备份存储放置策略**

一个GFS集群文件系统可能是多层分布的。一般情况下是成千上万个文件块 服务器分布于不同的机架上，而这些文件块服务器又被分布于不同机架上的客户来访问。因此，不同机架上的两台机器之间的通信可能通过一个或多个交换机。数据 块[冗余](https://baike.baidu.com/item/%E5%86%97%E4%BD%99)配置策略要达到连个目的：最大的数据可靠性和可用性，最大的网络带宽利用率。因此，如果仅仅把数据的拷贝置于不同的机器上很难满足这两个要求，必须 在不同的机架上进行[数据备份](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD)。这样即使整个机架被毁或是掉线，也能确保数据的正常使用。这也使数据传输，尤其是读数据，可以充分利用带宽，访问到多个机 架，而写操作，则不得不涉及到更多的机架。

**3.3 产生、重复制、重平衡**[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)

当MASTER产生新的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)时，如何放置新数据 块，要考虑如下几个因素：⑴尽量放置在磁盘利用率低的数据块服务器上，这样，慢慢地各服务器的磁盘利用率就会达到平衡。⑵尽量控制在一个服务器上 的“新创建”的次数。⑶由于上一小节讨论的原因，我们需要把[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)放置于不同的机架上。

MASTER在可用的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)备份低于用户设定的数 目时需要进行重复制。这种情况源于多种原因：服务器不可用，数据被破坏，磁盘被破坏，或者备份数目被修改。每个被需要重复制的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的优先级根据以下几项 确定：第一是现在的数目距目标的距离，对于能阻塞[用户程序](https://baike.baidu.com/item/%E7%94%A8%E6%88%B7%E7%A8%8B%E5%BA%8F)的数据块，我们也提高它的优先级。最后， MASTER按照产生[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的原则复制数据块，并把它们放到不同的机架内的服务器上。

MASTER周期性的平衡各服务器上的负载：它检查 chunk分布和[负载平衡](https://baike.baidu.com/item/%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1)，通过这种方式来填充一个新的服务器而不是把其他的内容统统放置到它上面带来大量的写数据。[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)放置的原则与上面讨论的相同， 此外，MASTER还决定那些数据块要被移除，原则上他会清除那些空闲空间低于平均值的那些服务器。

**3.4 垃圾收集**

在一个文件被删除之后，GFS并不立即收回磁盘空间，而是等到垃圾收集程序在文件和[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)级的的检查中收回。

当 一个文件被[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)删除之后，MASTER会立即记录下这些变化，但文件所占用的资源却不会被立即收回，而是重新给文件命了一个隐藏的名字，并附上了删除 的[时间戳](https://baike.baidu.com/item/%E6%97%B6%E9%97%B4%E6%88%B3)。在MASTER定期检查名字空间时，它删除超过三天（可以设定）的隐藏的文件。在此之前，可以以一个新的名字来读文件，还可以以前的名字恢复。当隐藏的文件在名字空间中被删除以后，它在内存中的元数据即被擦除，这就有效地切断了他和所有[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的联系。

在一个相似的定期的名字空间检查中，MASTER确认孤儿[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)（不属于任何文件）并擦除他的元数据，在和MASTER的心跳信息交换中，每个服务器报告他所拥有的数据块，MASTER返回元数据不在内存的数据块，服务器即可以删除这些数据块。

**3.5 过时数据的探测**

在[数据更新](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0)时如果服务器停机了，那么他所保存的数据备份就会过时。对每个[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)，MASTER设置了一个版本号来区别更新过的数据块和过时的数据块。

当MASTER 授权一个新的lease时，他会增加[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的版本号并会通知更新[数据备份](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD)。MASTER和备份都会记录下当前的版本号，如果一个备份当时不可用，那么他的 版本号不可能提高，当ChunkServer重新启动并向MASTER报告他的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)集时，MASTER就会发现过时的数据。

MASTER在 定期的垃圾收集程序中清除过时的备份，在此以前，处于效率考虑，在各客户机应答时，它会认为根本不存在过时的数据。作为另一个安全措施， MASTER在给客户及关于[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的应答或是另外一个读取数据的服务器数据是都会带上版本信息，在操作前客户机和服务器会验证版本信息以确保得到的是最新 的数据。

## 容错和诊断

**4.1 高可靠性**

⒋1.1 快速恢复

不管如何终止服务，MASTER和[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)服务器都会在几秒钟内恢复状态和运行。实际上，我们不对正常终止和不正常终止进行区分，服务器进程都会被切断而终止。客户机和其他的服务器会经历一个小小的中断，然后它们的特定请求超时，重新连接重启的服务器，重新请求。

⒋1.2 [数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)备份

如上文所讨论的，每个[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)都会被备份到放到不同机架上的不同服务器上。对不同的名字空间，用户可以设置不同的备份级别。在[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)服务器掉线或是数据被破坏时，MASTER会按照需要来复制数据块。

⒋1.3 MASTER备份

为 确保可靠性，MASTER的状态、操作记录和检查点都在多台机器上进行了备份。一个操作只有在[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)服务器硬盘上刷新并被记录在MASTER和其备份的上 之后才算是成功的。如果MASTER或是硬盘失败，系统[监视器](https://baike.baidu.com/item/%E7%9B%91%E8%A7%86%E5%99%A8)会发现并通过改变域名启动它的一个备份机，而客户机则仅仅是使用规范的名称来访问，并不会发 现MASTER的改变。

**4.2 **[数据完整性](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7)

每个[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)服务器都利用校验和来检验存储数据的完整性。原因：每个服务器随时都有发生崩溃的可能性，并且在两个服务器间比较数据块也是不现实的，同时，在两台服务器间拷贝数据并不能保证数据的一致性。

每个Chunk按64kB的大小分成块，每个块有32位的校验和，校验和和日志存储在一起，和用户数据分开。

在 读数据时，服务器首先检查与被读内容相关部分的校验和，因此，服务器不会传播错误的数据。如果所检查的内容和校验和不符，服务器就会给数据请求者返回一个 错误的信息，并把这个情况报告给MASTER。客户机就会读其他的服务器来获取数据，而MASTER则会从其他的拷贝来复制数据，等到一个新的拷贝完成 时，MASTER就会通知报告错误的服务器删除出错的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)。

附加写数据时的校验和计算优化了，因为这是主要的写操作。我们只是更新增加部分的校验和，即使末尾部分的校验和数据已被损坏而我们没有检查出来，新的校验和与数据会不相符，这种冲突在下次使用时将会被检查出来。

相反，如果是覆盖现有数据的写，在写以前，我们必须检查第一和最后一个[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)，然后才能执行写操作，最后计算和记录校验和。如果我们在覆盖以前不先检查首位[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)，计算出的校验和则会因为没被覆盖的数据而产生错误。

在空闲时间，服务器会检查不活跃的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)的校验和，这样可以检查出不经常读的数据的错误。一旦错误被检查出来，服务器会拷贝一个正确的[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)来代替错误的。

**4.3 诊断工具**

广 泛而细致的诊断日志以微小的代价换取了在问题隔离、诊断、[性能分析](https://baike.baidu.com/item/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90)方面起到了重大的作用。GFS服务器用日志来记录显著的事件（例如服务器停机和启动）和 远程的应答。远程日志记录机器之间的请求和应答，通过收集不同机器上的日志记录，并对它们进行分析恢复，我们可以完整地重现活动的场景，并用此来进行错误 分析。

## 测量

⒌1 [测试环境](https://baike.baidu.com/item/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83)

一台主控机，两台主控机备份，16台[数据块](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%9D%97)服务器，16台客户机。

每台机器：2块PⅢ1.4G处理器，2G内存，2块80G5400rpm的硬盘，1块100Mbps全双工网卡

19台服务器连接到一个HP2524[交换机](https://baike.baidu.com/item/%E4%BA%A4%E6%8D%A2%E6%9C%BA)上，16台客户机连接到另外一台交换机上，两台交换机通过1G的链路相连。