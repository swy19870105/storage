## 数据缩减技术效率对比

原创 *2016-05-19* *EMC中文技术社区* [EMC易安信中国技术社区](https://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651770916&idx=2&sn=2b1666e12a28e3cfb92bd24361164a89&scene=21##)

​      面对数据的急剧膨胀，企业需要不断购置大量的存储设备来应对不断增长的存储需求。然而，单纯地提高存储容量，这似乎并不能从根本解决问题。首先，存储设备的采购预算越来越高，大多数企业难以承受如此巨大的开支。其次，随着数据中心的扩大，存储管理成本、占用空间、制冷能力、能耗等也都变得越来越严重，其中能耗尤为突出。再者，大量的异构物理存储资源大大增加了存储管理的复杂性，容易造成存储资源浪费和利用效率不高。因此，我们需要另辟蹊径来解决信息的急剧增长问题，堵住数据“井喷”。

​      高效存储理念正是为此而提出的，它旨在缓解存储系统的空间增长问题，缩减数据占用空间，简化存储管理，最大程度地利用已有资源，降低成本。目前业界公认的五项高效存储技术分别是数据压缩、重复数据删除、自动精简配置、自动分层存储和存储虚拟化。数据压缩和重复数据删除是实现数据缩减的两种关键技术。简而言之，数据压缩技术通过对数据重新编码来降低冗余度，而重复数据删除技术侧重于删除重复的文件或数据块，从而实现数据容量缩减的目的。

​      本文将介绍数据压缩和重复数据删除技术的效率对比。

  [![img](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWRyj9aQ2xzibhDV2KkPMZPl6G2bMUTZfs37dKFs00J9eibaXBeEMxVVgqRiaBk7vVrqugA2g5PjVicWw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1)]()

 

**Lempel-Ziv系列压缩编码算法:**

 

​      数据压缩的起源可以追溯到信息论之父香农（Shannon）在1947年提出的香农编码。1952年霍夫曼（Huffman）提出了第一种实用性的编码算法实现了数据压缩，该算法至今仍在广泛使用。1977年以色列数学家Jacob Ziv 和Abraham Lempel提出了一种全新的数据压缩编码方式，Lempel-Ziv系列算法（LZ77和LZ78，以及若干变种）凭借其简单高效等优越特性，最终成为目前主要数据压缩算法的基础。LZ系列算法属于无损数据压缩算法范畴，采用词曲编码技术实现，目前主要包括LZ77、LZSS、LZ78和LZW四种主流算法。

​      Lempel-Ziv系列算法的基本思路是用位置信息替代原始数据从而实现压缩，解压缩时则根据位置信息实现数据的还原，因此又被称作"字典式"编码。目前存储应用中压缩算法的工业标准（ANSI、QIC、IETF、FRF、TIA/EIA）是LZS（Lempel-Ziv-Stac），由Stac公司提出并获得专利，当前该专利权的所有者是Hifn, Inc.。

​      数据压缩的应用可以显著降低待处理和存储的数据量，一般情况下可实现2:1 ~ 3:1的压缩比。

 

**文件级重复数据删除：**

 

​      文件级消重，通常也称为单实例存储（Single Instance），原理很简单。在文件系统中检查并判断两个文件是否完全相同，如果发现两个相同的文件，其中一个就会被指向另一个文件的指针所取代。

​      文件集消重的数据缩减效率通常在3:1的压缩比。

 

**数据块级重复数据删除：**

 

​      在备份、归档等实际的存储实践中，人们发现有大量的重复数据块存在，既占用了传输带宽又消耗了相当多的存储资源：有些新文件只是在原有文件上作了部分改动，还有某些文件存在着多份拷贝，如果对所有相同的数据块都只保留一份实例，实际存储的数据量将大大减少--这就是重复数据删除技术的基础。这一做法最早由普林斯顿大学李凯教授（DataDomain的三位创始人之一）提出，称之为全局压缩（Global Compression），并作为容量优化存储推广到商业应用。

​      重复数据删除是一种数据缩减技术，可对存储容量进行有效优化。它通过删除数据集中重复的数据，只保留其中一份，从而消除冗余数据，其原理如下图所示。消重技术可以有效提高存储效率和利用率，数据可以缩减到原来的1/20～1/50。这种技术可以很大程度上减少对物理存储空间的需求，减少传输过程中的网络带宽，有效节约设备采购与维护成本。

[![img](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWRyj9aQ2xzibhDV2KkPMZPljibwYVe2mgspgPrULE6wEwWYJxjGFDYmnibHmlTgnSV95LNQcJj93dhw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1)]()

​      数据块级的消重技术可以提供更高的数据消重率，因此目前主流的重复数据删除产品都是数据块级的。这些消重技术将文件分割成定长或变长的数据块，采用MD5/SHA1等Hash算法为数据块计算指纹（FP, Fingerprint）。可以同时使用两种及以上hash算法计算数据指纹，以获得非常小的数据碰撞发生概率。具有相同指纹的数据块即可认为是相同的数据块，存储系统中仅需要保留一份。这样，一个物理文件在存储系统就对应一个逻辑表示，由一组FP组成的元数据。当进行读取文件时，先读取逻辑文件，然后根据FP序列，从存储系统中取出相应数据块，还原物理文件副本。

 

**数据压缩与重复数据删除对比分析：**

 

​      数据压缩和重复数据删除技术都着眼于减少数据量，其差别在于数据压缩技术的前提是信息的数据表达存在冗余，以信息论研究作为基础;而重复数据删除的实现依赖数据块的重复出现，是一种实践性技术。然而，通过上面的分析我们发现，这两种技术在本质上却是相同的，即通过检索冗余数据并采用更短的指针来表示来实现缩减数据容量。它们的区别关键在于，消除冗余范围不同，发现冗余方法不同，冗余粒度不同，另外在具体实现方法有诸多不同。

1. **消除冗余范围**

   数据压缩通常作用于数据流，消除冗余范围受到滑动窗口或缓存窗口的限制。由于考虑性能因素，这个窗口通常是比较小的，只能对局部数据产生作用，对单个文件效果明显。重复数据删除技术先对所有数据进行分块，然后以数据块为单位在全局范围内进行冗余消除，因此对包含众多文件的全局存储系统，如文件系统，效果更加显著。如果把数据压缩应用于全局，或者把重复数据删除应用于单个文件，则数据缩减效果要大大折扣。

2. **发现冗余方法**

   数据压缩主要通过串匹配来检索相同数据块，主要采用字符串匹配算法及其变种，这是精确匹配。重复数据删除技术通过数据块的数据指纹来发现相同数据块，数据指纹采用hash函数计算获得，这是模糊匹配。精确匹配实现较为复杂，但精度高，对细粒度消除冗余更为有效；模糊匹配相对简单许多，对大粒度的数据块更加适合，精度方面不够。

3. **冗余粒度**

   数据压缩的冗余粒度会很小，可以到几个字节这样的小数据块，而且是自适应的，不需要事先指定一个粒度范围。重复数据删除则不同，数据块粒度比较大，通常从512到8K字节不等。数据分块也不是自适应的，对于定长数据块需要事先指定长度，变长数据分块则需要指定上下限范围。更小的数据块粒度会获得更大的数据消冗效果，但计算消耗也更大。

4. **性能瓶颈**

   数据压缩的关键性能瓶颈在于数据串匹配，滑动窗口或缓存窗口越大，这个计算量就会随之大量增加。重复数据删除的性能瓶颈在于数据分块与数据指纹计算，MD5/SHA-1等hash函数的计算复杂性都非常高，非常占用CPU资源。另外，数据指纹需要保存和检索，通常需要大量内存来构建hash表，如果内存有限则会对性能产生严重影响。

5. **数据安全**

   这里的数据压缩都是无损压缩，不会发生数据丢失现象，数据是安全的。重复数据删除的一个问题是，利用hash产生的数据块指纹可能会产生的碰撞，即两个不同的数据块生成了相同的数据指纹。这样，就会造成一个数据块丢失的情况发生，导致数据发生破坏。因此，重复数据删除技术存在数据安全隐患。

6. **应用角度**

   数据压缩直接对流式数据进行处理，不需要事先对全局信息进行分析统计，可以很好地利用流水线或管道方式与其他应用结合使用，或以带内方式透明地作用于存储系统或网络系统。重复数据删除则需要对数据进行分块处理，需要对指纹进行存储和检索，需要对原先物理文件进行逻辑表示。如果现有系统要应用这种技术，则需要对应用进行修改，难以做到透明实现。目前重复数据删除并不是一个通常功能，而更多地以产品形态出现，如存储系统、文件系统或应用系统。因此，数据压缩是一种标准功能，而重复数据删除现在还没有达到这种标准，应用角度来看，数据压缩更为简单。