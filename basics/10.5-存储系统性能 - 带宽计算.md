## 存储系统性能 - 带宽计算

原创 *2016-06-10* *EMC中文技术社区* [EMC易安信中国技术社区](https://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771011&idx=1&sn=c2fcb842326c30f74c10eba100ea54c9&scene=21##)

​     遇到过很多同行、客户问我：“xxx存储系统究竟最大支持多少【IOPS】？”，这真不好说，因为手里确实没有测试数据。更何况，IOPS与i/o size、random/sequential、read/write ratio、App threading-model、response time baseline等诸多因素相关，这些因素组合起来便可以描述一种类型的I/O，我们称之为【I/O profile】。不同的因素组合得到的IOPS都不一样，通常我们看到的【标称IOPS】都是在某一个固定组合下测得的，拿到你自己的生产环境中，未必能达到标称值。这也是为什么要做前期的performance analysis/sizing的缘故。

 

​     直到有人这样问我：“xxx存储系统究竟最大支持多少【带宽】？"我愣了下，仔细想想，硬件性能极限就摆在那，基于bandwidth = Frequency * bit-width，而且很多需要的数据都是公开的，东拼西凑应该可以算出个大概。

我并不是Performance专家，从未做过Performance Consulting/Sizing方面的工作，最多也只是做过性能方面的分析/排错，所以这篇文章的准确性多半存在不靠谱的地方，读者斟酌着看吧。

​     在读文章之前，建议先看一下如下计算公式和名词。

**计算公式：**

- Real-world result = nominal * 70% -> 我所标称的数据都是*70%（[性能计算：Little Law & Utilization Law](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=10015548&idx=3&sn=9730013a0333256e43489059f61ea457&scene=21#wechat_redirect)）以尽可能接近实际数据，但如果另外提供了由资料获得的更为准确的数据，则以其为准。
- Bandwidth = frequency * bit-width

​     QPI带宽：假设QPI频率==2.8 Ghz

​     × 2 bits/Hz (double data rate)

​     × 20 (QPI link width)

​     × (64/80) (data bits/flit bits)

​     × 2 (unidirectional send and receive operating simultaneously)

​     ÷ 8 (bits/byte)

​     = 22.4 GB/s

 

**术语：**

- Westmere -> Intel CPU微架构的名称
- GB/s -> 每秒传输的byte数量
- Gb/s -> 每秒传输的bit数量
- GHz -> 依据具体操作而言，可以是单位时间内运算的次数、单位时间内传输的次数 (也可以是GT/s)
- 1byte = 8bits
- IOH -> I/O Hub，处于传统北桥的位置，是一颗桥接芯片。
- QPI -> QuickPathInterconnect，Intel前端总线（FSB）的替代者，可以认为是AMD Hypertransport的竞争对手
- MCH -> Memory Controller Hub，内置于CPU中的内存控制器，与CPU直接通信，无需走系统总线
- PCI Express(Peripheral Component InteconnectExpress, PCIe) - 一种计算机扩展总线(Expansion bus)，实现外围设备与计算机系统内部硬件（包括CPU和RAM）之间的数据传输。
- Overprovisioning - 比如 48*1Gbps access port交换机，通常只有4*1Gbps uplink，那么overprovisioning比 = 12:1
- PCI-E 2.0每条lane的理论带宽是500MB/s
- X58 – 相当于传统的北桥，只不过不再带有内存控制器，Code name = Tylersburg
- Lane - 一条lane由一对发送/接收差分线(differential line)组成，共4根线，全双工双向字节传输。一个PCIe slot可以有1-32条lane，以x前缀标识，通常最大是x16。
- Interconnect - PCIe设备通过一条逻辑连接(interconnect)进行通信，该连接也称为Link。两个PCIe设备之间的link是一条点到点的通道，用于收发PCI请求。从物理层面看，一个link由一条或多条Lane组成。低速设备使用single-lane link，高速设备使用更宽的16-lane link。

 

**相关术语：**

- address/data/control line
- 资源共享 ->资源仲裁
- 时钟方案（Clock Scheme）
- Serial Bus

**PCI-E Capacity:**

Per lane (each direction):

- v1.x: 250 MB/s (2.5 GT/s)
- v2.x: 500 MB/s (5 GT/s)
- v3.0: 1 GB/s (8 GT/s)
- v4.0: 2 GB/s (16 GT/s)

16 lane slot (each direction):

- v1.x: 4 GB/s (40 GT/s)
- v2.x: 8 GB/s (80 GT/s)
- v3.0: 16 GB/s (128 GT/s)

 

​     性能是【端到端】的，中间任何一个环节都有自己的性能极限，它并不像一根均匀水管，端到端性能一致。存储系统显然是不均衡的 ->overprovisioning。我将以中端存储系统为例，高端存储过于复杂，硬件结构可能都是私有的，而中端系统相对简单，就以一种双控制器、SAS后端、x86架构的存储系统为例。为了方便名称引用，我们就称他为Mr.Block_SAN吧。

 

​     控制器上看得见摸得着，又可以让我们算一算的东西也就是CPU、内存、I/O模块，不过我今天会带上一些极为重要但却容易忽略的组件。先上一张简图（字难看了点，见谅），这是极为简化的计算机系统构成，许多中端存储控制器也就这么回事儿。

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

​     CPU - 假设控制器采用Intel Xeon-5600系列处理器（Westmere Microarchiecture ），例如Xeon 5660，支持DDR3-1333。CPU Bandwidth = 2.8GHz * 64bits / 8 =22.4 GB/s。

 

​     内存 – Mr.Block_SAN系统通过DMA (Direct Memory Access) 直接在Front End，内存以及Back end之间传输数据。因此需要知道内存是否提供了足够的带宽。3* DDR3，1333MHz带宽==29GB/s（通常内存带宽都是足够的），那么bit width应该是64bits。Westmere集成了内存控制器，因此极大的降低了CPU与内存通信的延迟。Mr.Block_SAN采用【X58 IOH】替代原始的北桥芯片，X58 chipset提供36 lane PCIe2.0 = 17.578GB/s bandwidth（后面会有更多解释）。

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

​     I/O模块 （SLIC）- SLIC是很多人关心的，因为它直接收/发送I/O。需要注意的是一个SLIC所能提供的带宽并不等于其所有端口带宽之和，还要看控制芯片和总线带宽。以SAS SLIC为例，一个SAS SLIC可能由两个SAS Controller组成，假设每个SAS Controller带宽大约2200MB/s realworld，一个SAS port = 4 * 6Gbps /8 * 70% =2100MB/s；一个SAS Controller控制2*SAS port，可见单个SAS Controller无法处理两个同时满负荷运转的SAS port（2200MB/s < 4200MB/s），这里SAS Controller是个瓶颈-> Overprovisioning!整个SAS SLIC又是通过【x8 PCI-E 2.0】 外围总线与【IOH】连接。x8  PCIe bandwidth = 8 * 500MB/s * 70% = 2800MB/s。如果两个SAS Controller满负荷运作的话，即4400MB/s > 2800MB/s，此时x8 PCIe总线是个瓶颈 -> Overprovisioning!

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

​     其实还可以计算后端磁盘的带宽和，假设一个Bus最多能连250块盘，若是SAS 15K RPM则提供大约12MB/s的带宽（非顺序随机64KB，读写未知），12 * 250 = 3000MB/s > 2100MB/s -> Overprovisioning!

 

​     Tip：一个SAS Controller控制两个SAS Port，所以如果只需要用到两根bus，可以错开连接端口，从而使的得两个SAS Controller都能得到利用。

 

​     同理，对任何类型的SLIC，只要能够获得其端口速率、控制器带宽、PCIe带宽，即可知道瓶颈的位置。我选择算后端带宽的原因在于，前端你可以把容量设计的很大，但问题是流量过来【后端】能否吃下来？Cache Full导致的Flush后端能否挡住？对后端带宽是个考验，所以以SAS为例或许可以让读者联想到更多。

 

​     PCI-Express – PCIe是著名的外围设备总线，用于连接高带宽设备与CPU通信，比如存储系统的I/O模块。X58提供了36 lane PCIe 2.0，因此36*500/1024 = 17.578125GB/s带宽。

 

​     QPI & IOH – QPI通道带宽可以通过计算公式获得，我从手中资料直接获得的结果是19-24GB/s（运行在不同频率下的值）。IOH芯片总线频率是12.8GB/s （List of Intel chipsets这里获得，但不确定总线频率是否就是指IOH本身的运行频率）< 17.578GB/s（36 Lane） -> Overprovisioning!

 

​     OK，算完了，能回答Mr.Block_SAN最大能提供多少带宽了吗？看下来CPU、RAM、QPI的带宽都上20GB/s，留给前后端的PCIe总线总共也只有18GB/s不到，即便这样也已经overload了IOH（12GB/s）。所以看来整个系统的瓶颈在IOH，只有12GB/s。当然，你还得算一下Mr.Block_SAN是否支持足够多的外围设备（eg. I/O模块）来完全填充这12GB/s，如果本身就不支持那么多外围设备，那IOH也算不上是瓶颈了。另外，我看到已经有网友提出我的计算存在8b/10b编码换算错误，由于个人对硬件系统编码尚未透彻研究，理解这部分的读者可以自己对相应组件再乘以80%（我记得应该是）去掉编码转换的开销。

​     这篇文章更多的是一种举例式的说明，其中的数字和组件存在假设的情况。大家在计算的时候，可以参考这个思路将自己系统的参数和组件套用上去，从而计算出自己系统的带宽瓶颈。

​     注意     下图有点旧了，我把PCIe 36 Lane框成了MAX Bandwidth，因为那个时候以为IOH应该有足够的带宽，但后来发现可能不是这样，但图已经被我擦了，所以就不改了。

[![img](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWYic4OUw50MP0iaaD65RraK78ibrNt6pficEmMPS6ylb7fuZ4gQHgvMDiaxLqr716zNqtiamxdiblJ0kCbw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1)]()