## 对比ScaleIO和VMware的VSAN技术

原创 *2016-06-27* *EMC中文技术社区* [EMC易安信中国技术社区](https://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771095&idx=2&sn=c52ef484ef943f41549f0115aad334df&scene=21##)

目前企业级存储应用有二大趋势。1、随着云计算性能、速度和弹性的提高，以及托管费用的降低，越来越多的中小型企业纷纷投向公有云服务。2、金融公司、大型企业和政府部门则通过EMC、VMware、CISCO、IBM等IT巨头提供的解决方案部署自己的私有云。

 

随着私有云规模的扩张以及越来越多中小企业将应用迁向公有云，传统的存储管理技术面临的成本和管理挑战已经越来越大。虚拟SAN就是一种新兴的存储解决方案，它可以利用软件将应用服务器的硬盘组织成一个共享存储，从而现实了软件定义存储。本文将分别介绍ScaleIO和VMware的虚拟SAN技术，然后进行参数和应用范围对比。

 

**ScaleIO的VSAN技术**

 

ScaleIO的VSAN技术叫ECS，该技术可以利用数据中心里的数千台服务器构建成一个可扩展的文件系统，而且新旧服务器都支持。企业无需雇佣具有特殊技术才能的存储管理员或增加现有服务器管理员的工作量，即可使用ScaleIO的ECS技术实现上述目标。企业客户可以使用现有的服务器，或者新购买的服务器，他们无需购买主机总线适配器、交换机或是SAN，他们可以在他们选定用来运行关键商业应用的服务器上建立一个虚拟SAN。他们可以在服务器上同时运行他们自己的商业应用和我们的软件，建立一个全新的SAN。

 

 

ScaleIO系统在搭建时需要至少三个SDS服务器，该系统主要由硬件和软件二部分组成：

 

**硬件：**

硬件通常是指所在数据中心现有的应用程序服务器或者新的节点集。ScaleIO系统的硬件包括二部分：

 

- **节点**

节点或服务器用于安装和运行ScaleIO系统。ScaleIO 1.2版本兼容的操作系统包括Linux CentOS 6.0及以上版本、Linux Red Hat 6.0及以上版本、VMware ESX 5.0或5.1和Xen Server 6.1版本。

 

- **存储介质**

ScaleIO支持的存储介质包括：HDD、SSD、PCIe闪存卡，也支持DAS或者外部连接。

 

 

**软件：**

ScaleIO系统主要包含以下组件：

 

- **Mete Data Manager（MDM）**

MDM用于配置和监控ScaleIO系统。MDM可以配置在三个成员（需三台服务器）的冗余集群模式或者一个成员（需一台服务器）的单一模式。

 

- **ScaleIO Data Server（SDS）**

SDS用于管理单个服务器的容量，供前端数据访问。SDS软件需要安装在为ScaleIO系统提供存储的所有服务器上，这些服务器通过SDS被其它设备访问。

 

- **ScaleIO Data Client（SDC）**

SDC是一个轻量级的设备驱动程序，用于将ScaleIO卷转换成块设备，然后供安装了SDC的服务器使用。

 

 

下图展示了一套ScaleIO系统的解决方案，该方案中包括三个安装了ScaleIO套件的服务器。

 

[![img](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIVxaDkVPKlvPgb0BXc95vKzP8aiar9GTD6AQb5yzLyuU7wA7ic2Fxs7h4QIEgFpUQr8o5G5IDg1uePQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)]()

 

在配置ScaleIO系统前，有二个主要的概念需要用户了解：保护域（Protection Domain）和存储池（Storage Pool）。这二个概念是ScaleIO系统中物理层和虚拟层之间的桥梁。

 

**保护域（Protection Domain）**

每个保护域（Protection Domain）都包含一组SDS组件，每个SDS组件只能属于一个保护域。因此，每个保护域是一组唯一的SDS组件。

 

 

**存储池（Storage Pool）**

存储池（Storage Pool）是保护域（Protection Domain）中的一组物理存储设备，每个物理设备只属于一个存储池。默认情况下，一个保护域中只有一个存储池，用户可以自定义设置。下图中，一个保护域中有二个存储池。

 

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

**ScaleIO应用**

 

ScaleIO系统的安装过程非常简单。一般情况下分为二个步骤，首先建立物理存储层，然后配置虚拟SAN。

 

**（一）建立物理存储层**

物理层是由硬件和ScaleIO软件组成，用户可以通过下列步骤完成物理层安装：

 

1.确认集群MDM方案。用户可以选择三节点的冗余管理方案或者单一节点方案，然后安装MDM组件。

2.确认ScaleIO系统中需要使用的所有节点，然后在这些节点上安装SDS组件。

3.确认所有需要访问虚拟SAN的节点，然后在这些节点上安装SDC组件。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

 

ScaleIO系统的节点间将利用现有局域网进行通信，使用标准TCP/IP协议。SDS可以有多个IP地址，以便提供更宽的带宽和更好的冗余。

 

 

**（二）配置虚拟SAN**

 

在ScaleIO系统中，MDM用于配置虚拟SAN。它首先将汇聚所有SDS组件来生成一个虚拟SAN存储，然后将卷定义到存储池（Storage Pool），最后导出给安装了SDC的服务器使用。用户需要通过下列步骤将虚拟SAN导出：

 

1.定义卷。每个卷都是均匀的分布在存储池上，存储池是保护域（Protection Domain）中的一组物理存储设备，通过RAID保护方案确保数据安全。

2.映射卷。指定SDC组件可以访问的卷，SDC和卷在映射时没有限制，可以将所有卷映射到所有SDC上面。

 

 

**配置案例**

 

本案例将演示如何为ScaleIO 1.2版本的系统添加卷。

 

1．完成物理层安装；

2．在ScaleIO系统上创建卷，运行下列命令：

 

[root@rhel02 ~]# scli --mdm_ip 192.168.1.130 --add_volume--protection_domain_name pd2 --storage_pool_nameHDD --size_gb 30 --volume_namepd2_hdd_vol2

 

本例中参数如下：

•   MDM主机IP地址：192.168.1.130

•   保护域：pd2

•   存储池：HDD

•   卷大小：30GB

•   卷名称：pd2_hdd_vol2

 

 

3. 映射卷到SDC，运行下列命令：

 

[root@rhel02 ~]# scli --mdm_ip 192.168.1.130 --map_volume_to_sdc--volume_name pd2_hdd_vol2 --sdc_ip192.168.1.101

 

本例中参数如下：

•   MDM主机IP地址：192.168.1.130

•   映射主机IP地址：192.168.1.101

 

 

4. 挂载卷

 

当卷映射到SDC成功后，就可以在Linux主机上进行卷挂载，命令如下：

 

[root@rhel01 ~]# mkfs -t ext4 /dev/scinib

[root@rhel01 ~]# mkdir /mnt/vol2

[root@rhel01 ~]# mount /dev/scinib /mnt/vol2



**VMware的虚拟SAN**

 

VMware在vSphere 5.5中引入了Virtual SAN技术，迈出了实现存储虚拟化的第一步。VMware VSAN将 vSphere 集群中的主机本地连接的磁盘聚合起来，然后创建一个分布式共享存储，它可实现在 VMware vCenter中快速调配存储，作为虚拟机创建和部署操作的一部分。

 

VMware VSAN是一种混合磁盘系统，它利用聚合本地固态驱动器 (SSD) 作为缓存，同时结合使用本地硬盘驱动器 (HDD)来提供可供虚拟机使用的集群式数据存储。这样既能实现企业级性能又能提供具有强大恢复能力的存储平台。在 VMware VSAN 环境中，需要配置若干 ESXi 主机以形成一个 VMware VSAN 集 群。所有 ESXi 主机均通过VMware Virtual SAN专用网络进行通信。大部分主机均需要配备本地硬盘驱动器，其中大部分配有本地固态驱动器。未配备本地磁盘驱动器的主机可以共享计算资源并利用集群式存储资源。本地固态驱动器可以优化所有主机的存储使用效率。通过组合每台主机上的本地硬盘驱动器和固态驱动器，最多可形成 5 个本地磁盘组。每个磁盘组只能使用一个 SSD，但最多可以使用5个硬盘驱动器。

[![img](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIVxaDkVPKlvPgb0BXc95vKz9631ibYv7YbvXySyn691rkXl0eNcLkrB3J9XlNqicgia8DjZv2IAff1Zg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)]()

 

**VMware VSAN 实施要求：**

—— 一个群集配置至少三台主机

—— 所有三台主机都必须提供存储

- vSphere 5.5 U1 或更高版本

—— 本地连接的磁盘

- 磁盘 (HDD)
- 基于闪存的设备 (SSD)

—— 网络连接

- 1GB 以太网
- 10GB 以太网（首选）

 

 

创建磁盘组需要指定一个基于闪存的设备（SAS、SATA、或 PCIe 固态硬盘）以及一个或多个磁盘（SAS 或 SATA 硬盘）。磁盘组构成了Virtual SAN 数据存储的分布式闪存层，并为其提供存储容量。Virtual SAN 分布式闪存层通过在所有磁盘前端提供读缓存和写缓冲，优化了虚拟机和应用的性能。存储容量分为两部分：70% 用于读缓存，30% 用于写缓冲。所有磁盘组先通过改进的磁盘上文件系统VMware vSphere VMFS-L 进行格式化，然后作为单个数据存储装载到对象存储文件系统数据存储中。每个磁盘的VMFS-L 格式化共占用750 MB 的容量。

磁盘组、设备和开销表：

| 项目                      | 最小          | 最大          |
| ----------------------- | ----------- | ----------- |
| 磁盘组                     | 磁盘组每个主机上一个  | 每个主机上五个     |
| 闪存设备：SAS、SATA、PCIe 固态硬盘 | 每个磁盘组一个     | 每个磁盘组一个     |
| 磁盘设备                    | 每个磁盘组一个硬盘   | 每个磁盘组七个硬盘   |
| 磁盘格式化开销                 | 每个硬盘 750 MB | 每个硬盘 750 MB |

 

通过组合VMware Virtual SAN集群中所有ESXi主机的磁盘组，可创建一个VMware Virtual SAN数据存储。每个VMware Virtual SAN集群只有一个数据存储，因此它包含了该集群中的所有HDD 和 SSD 资源。通过对象存储文件系统（OSFS），所有主机上的VMFS卷可以合并为一个数据存储进行安装。此数据存储包含所有虚拟机文件 （包括 vmdk 文件）。每个Vmdk文件可创建不同的虚拟机存储策略, 用于定义数据存储中数据在磁盘上的存储方式。通过配置这些虚拟机存储策略可充分利用VMware Virtual SAN 的功能。

 

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

 

VMware Virtual SAN 数据存储可在以下方面帮助管理员使用软件定义的存储:

 

- 存储策略按虚拟机的体系结构配置：每个数据存储可配置多个策略使得每个虚拟机可以拥有不同的存储。
- vSphere 和 vCenter 集成：VMware Virtual SAN 功能是内置的,无需任何虚拟设备。如同vSphere HA 或 DRS 一样,您可以创建一个 VMware Virtual SAN 集群。
- 横向扩展存储：一个集群最多可以包含8台ESXi主机。通过在集群中添加新节点或将VMware Virtual SAN设置为扫描并自动添加空磁盘，可以实现扩展。
- 内置恢复能力：具有一条默认策略，系统会镜像所有未针对VMware Virtual SAN配置的虚拟机对象。
- SSD 缓存：在写入 HDD 前，会将所有 I/O 转入 SSD 并进行缓存。
- 聚合式计算和存储：甚至那些没有本地存储的虚拟机也可以利用VMware Virtual SAN存储资源。

[![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)]()

 

**VMware VSAN 的配置步骤：**

首先，配置用于 VMware VSAN 的 VMkernel 网络，并允许所有主机访问该网络。

 

然后，创建一个集群并针对 VMware VSAN 启用该集群。将所有主机添加到这一VMware VSAN 集群中。

可以采用“手动”或“自动”模式配置VMware VSAN 集群。 如果在“自动”模式下配置VMware VSAN，则VMware VSAN会要 求使用所有本地磁盘来创建分布式Virtual SAN 数据存储。如果在“手动”模式下配置VMware VSAN，则必须通过创建 “磁盘组” 手动选择要添加分布式Virtual SAN 数据存储的磁盘。默认的模式为自动模式。VMware VSAN会扫描所有主机中的空磁盘。当它找到这些空 磁盘时,会对其进行配置以用于VMware VSAN。

 

在集群中启用 VMware VSAN 时，会创建单一的Virtual SAN 数据存储。此数据存储可 以使用集群中所有主机的存储组件。使用对象存储文件系统（OSFS）可以安装存储。VMware VSAN 在VSAN 数据存储中以灵活的数据容器的形式存储和管理数据。对象是一个逻辑卷，该逻辑卷有自己的分布式数据和元数据，并且可以跨整个集群进行访问。在ESXi 存储堆栈中, 这些对象以设备的形式出现。尽管只为整个VMware VSAN 集群创建了一个Virtual SAN 数据存储，但该数据存储可 以有多个与之关联的存储策略。这些存储策略可配置不同的存储功能。

 

 

 

**ScaleIO和VMware的VSAN参数对比**

 

ScaleIO的ECS和VMware VSAN基本上做同样的事情，通过一组主机的存储来建立虚拟SAN，然后在集群内共享。ScaleIO是一个管理程序，支持不同操作系统的物理服务器，可以扩展到数百台机器。VMware VSAN只能应用于ESXi主机，被集成到vSphere内核中，只能最多扩展到32个节点。另外，二项技术以不同的方式来使用混合存储，结构完全不同。下表对比了ScaleIO的ECS和VMware VSAN的参数：

​              

|          | VMware              | ScaleIO 1.2版本                            |
| -------- | ------------------- | ---------------------------------------- |
| 节点       | 3台 —— 32台           | 3台 —— 大于100台                             |
| 网络类型     | VMware VSAN专用网络     | IP网络                                     |
| 存储介质     | 本地磁盘                | 本地磁盘                                     |
| 是否支持VIPR | 支持                  | 支持                                       |
| 对象类型     | vSphere 5.5 U1或更高版本 | Linux CentOS  6.0及以上版本、Linux   Red Hat 6.0及以上版本、VMware   ESX 5.0或5.1和Xen Server 6.1版本 |

 